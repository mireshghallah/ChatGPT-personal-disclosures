{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "sns.set(style='ticks', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_mean(df, by, column, rot=0):\n",
    "    df2 = pd.DataFrame({col:vals[column] for col, vals in df.groupby(by)})\n",
    "    means = df2.mean().sort_values(ascending=False)\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_path = ''  # INSERT YOUR OUTPUT DIRECTORY PATH HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = '' # INSERT YOUR OPENAI API KEY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br> \n",
    "\n",
    "## **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(output_directory_path + '/sampled_data.csv')\n",
    "len(data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'conversation_hash', 'model', 'timestamp', 'conversation',\n",
       "       'turn', 'language', 'openai_moderation', 'detoxify_moderation', 'toxic',\n",
       "       'redacted', 'state', 'country', 'hashed_ip', 'header',\n",
       "       'conversation_simplified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_conversation_dict = {r['conversation_hash']: r['conversation_simplified'] for i, r in data_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "## **Predict tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run over the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = np.array_split(data_df, 100)\n",
    "\n",
    "for j, _df in enumerate(df_list):\n",
    "\n",
    "    if not os.path.exists(output_directory_path + '/user-task-prediction/user_tasks.gpt4.' + str(j) + '.csv'):\n",
    "\n",
    "        print(datetime.now(), '\\t', j)\n",
    "\n",
    "        _output_dicts = []\n",
    "\n",
    "        for i, r in _df.iterrows():\n",
    "\n",
    "            _wildchat_query = r['conversation_simplified']\n",
    "\n",
    "            if len(_wildchat_query) > 5000:\n",
    "                _wildchat_query = _wildchat_query[:5000]\n",
    "            \n",
    "            _prompt = \"\"\"Read the following conversation between a user and an AI chatbot. Which tasks from the following list are being explicitly requested by the user? For each task, list the task, your confidence, and your reasoning and evidence. \n",
    "\n",
    "Example:\n",
    "[{\"task\": \"summarization\", \"confidence\": \"high confidence\", \"reasoning_and_evidence\": \"the user asks for a summary of a text\"}, \n",
    " {\"task\": \"explanation\", \"confidence\": \"medium confidence\", \"reasoning_and_evidence\": \"the user asks for a description of how the methods works and the chatbot replies with a description\"}]\n",
    "\n",
    "Tasks: \n",
    "- summarization\n",
    "- model jailbreaking (e.g. asking model to roleplay as DAN, NsfwGPT, Niccolo Machiavelli, IMMORAL, AIM, or Kevin)\n",
    "- generating prompts for AI models\n",
    "- story and script generation\n",
    "- song and poem generation\n",
    "- generating character descriptions\n",
    "- code generation\n",
    "- code editing and debugging\n",
    "- generating communications (email, text messages, etc.)\n",
    "- generating non-fictional documents (resumes, essays, etc.)\n",
    "- editing existing text\n",
    "- comparison, ranking, and recommendation\n",
    "- brainstorming and generating ideas\n",
    "- information retrieval \n",
    "- solving logic, math, and word problems\n",
    "- explanation, how-to, practical advice\n",
    "- personal advice about mental health, relationships, etc.\n",
    "- back-and-forth role-playing with the user\n",
    "- answering multiple choice question\n",
    "- translation\n",
    "- general chitchat\n",
    "\n",
    "Conversation:\n",
    "\\\"\"\"\"\n",
    "\n",
    "            _prompt += _wildchat_query\n",
    "            _prompt += \"\"\"\\\"\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            _response = openai.ChatCompletion.create(model=\"gpt-4\",\n",
    "                                                    messages=[{\"role\": \"user\", \"content\": _prompt}])\n",
    "            _answer = _response['choices'][0]['message']['content']\n",
    "            \n",
    "            _output_dicts.append({'conversation_hash': r['conversation_hash'],\n",
    "                                  'conversation_simplified': r['conversation_simplified'],\n",
    "                                  'response': _response,\n",
    "                                  'answer': _answer})\n",
    "\n",
    "        _output_df = pd.DataFrame(_output_dicts)\n",
    "        _output_df.to_csv(output_directory_path + '/user-task-prediction/user_tasks.gpt4.' + str(j) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = []\n",
    "for _file_name in os.listdir(output_directory_path + '/user-task-prediction'):\n",
    "    if _file_name.endswith('.csv') and _file_name.startswith('user_tasks.gpt4'):\n",
    "        df_list.append(pd.read_csv(output_directory_path + '/user-task-prediction/' + _file_name))\n",
    "len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tasks_df = pd.concat(df_list)\n",
    "len(predicted_tasks_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_data_dict = defaultdict(list)\n",
    "data_dicts = []\n",
    "\n",
    "for i, r in predicted_tasks_df.iterrows():\n",
    "\n",
    "    try:\n",
    "\n",
    "        _answer = r['answer'].strip()\n",
    "\n",
    "        if _answer.endswith(','):\n",
    "            _answer = _answer.strip(',')\n",
    "\n",
    "        if len(_answer.split('\\n')) > 1:\n",
    "            _lines = [l for l in _answer.split('\\n') if l.strip()]\n",
    "            _formatted_lines = []\n",
    "            for l in _lines[:-1]:\n",
    "                l = l.strip()\n",
    "                if not l.endswith(','):\n",
    "                    # print(l)\n",
    "                    _formatted_lines.append(l + ',')\n",
    "                else:\n",
    "                    _formatted_lines.append(l)\n",
    "            _formatted_lines.append(_lines[-1])\n",
    "            _answer = ' '.join(_formatted_lines)\n",
    "\n",
    "        if _answer.startswith('{'):\n",
    "            _answer = '[' + _answer\n",
    "        if _answer.endswith('}'):\n",
    "            _answer = _answer + ']'\n",
    "        if _answer.endswith(', ]'):\n",
    "            _answer = _answer[:-3] + ']'\n",
    "        _answer = _answer.replace('\\'reasoning_and_evidence\\': ', '\"reasoning_and_evidence\": ')\n",
    "        _answer = _answer.replace(': \\'', ': \"')\n",
    "        _answer = _answer.replace('\\'}', '\"}')\n",
    "\n",
    "        _json_answer = json.loads(_answer)\n",
    "\n",
    "        for _item in _json_answer:\n",
    "\n",
    "            _confidence = _item['confidence']\n",
    "            _reason = _item['reasoning_and_evidence']\n",
    "            _category = _item['task'].lower()\n",
    "\n",
    "            if _category:\n",
    "                conversation_data_dict[r['conversation_hash']].append({'conversation_hash': r['conversation_hash'],\n",
    "                                                                       'predicted_confidence': _confidence,\n",
    "                                                                       'predicted_reason': _reason,\n",
    "                                                                       'predicted_category': _category})\n",
    "                data_dicts.append({'conversation_hash': r['conversation_hash'],\n",
    "                                   'conversation_simplified': r['conversation_simplified'],\n",
    "                                   'predicted_confidence': _confidence,\n",
    "                                   'predicted_reason': _reason,\n",
    "                                   'predicted_category': _category})\n",
    "                \n",
    "    except:\n",
    "        print(_answer)\n",
    "        print()\n",
    "        x = 0\n",
    "\n",
    "    \n",
    "                \n",
    "formatted_predicted_tasks_df = pd.DataFrame(data_dicts)\n",
    "            \n",
    "len(conversation_data_dict), len(formatted_predicted_tasks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predicted_tasks_df.to_csv(output_directory_path + '/user-task-prediction/tasks.gpt4.processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "## **Predict sensitive categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run over the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = np.array_split(data_df, 500)\n",
    "\n",
    "for j, _df in enumerate(df_list):\n",
    "\n",
    "    if not os.path.exists(output_directory_path + '/sensitive-category-prediction/sensitive_categories.gpt4.' + str(j) + '.csv'):\n",
    "\n",
    "        print(datetime.now(), '\\t', j)\n",
    "\n",
    "        _data_dicts = []\n",
    "\n",
    "        for i, r in _df.iterrows():\n",
    "\n",
    "            _wildchat_query = r['conversation_simplified']\n",
    "            _wildchat_query = '\\n'.join([l for l in _wildchat_query.split('\\n') if len(l.strip()) > 5 and l.startswith('USER:')])\n",
    "            if len(_wildchat_query) > 5000:\n",
    "                _wildchat_query = _wildchat_query[:5000]\n",
    "\n",
    "            if _wildchat_query.strip():\n",
    "        \n",
    "                _prompt = \"\"\"In the following messages, does the user reveal any of these categories of information? For each category that is included, list the category, whether or not it is explicit or implicit, who if anyone is explictly involved, your confidence, and your reasoning. If a category is not present, do not list it.\n",
    "\n",
    "Example:\n",
    "2. Quoted email or messaging transcripts; explicit; the user, the user's coworker; very confident; the query contains the text of an email message.\n",
    "\n",
    "Categories:\n",
    "1. Sexually explicit or sexually suggestive content\n",
    "2. Copy-and-pasted emails or messaging transcripts\n",
    "3. Copy-and-pasted programming code longer than a single line\n",
    "4. Job, visa, and other applications\n",
    "5. Financial or corporate information\n",
    "6. Academic and educational information, including homework problems and essay writing\n",
    "7. Personal expressions about the user's emotions, mental health, etc.\n",
    "8. Personal expressions about the user's politics and religion\n",
    "9. Mention or discussion of the user's personal relationships\n",
    "10. Medical and healthcare information\n",
    "11. Engagement with a specific fandom, including character development, story writing, and discussions related to the fandom\n",
    "12. Mention or discussion of the user's hobbies and habits\n",
    "\n",
    "Messages:\n",
    "\\\"\"\"\"\n",
    "                _prompt += _wildchat_query\n",
    "                _prompt += \"\"\"\\\"\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "                _response = openai.ChatCompletion.create(model=\"gpt-4\",\n",
    "                                                         messages=[{\"role\": \"user\", \"content\": _prompt}],\n",
    "                                                         temperature=1)\n",
    "                _answer = _response['choices'][0]['message']['content']\n",
    "                \n",
    "                _data_dicts.append({'conversation_hash': r['conversation_hash'],\n",
    "                                    'conversation_simplified': r['conversation_simplified'],\n",
    "                                    'response': _response,\n",
    "                                    'answer': _answer})\n",
    "\n",
    "        _data_df = pd.DataFrame(_data_dicts)\n",
    "        _data_df.to_csv(output_directory_path + '/sensitive-category-prediction/sensitive_categories.gpt4.' + str(j) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "### **Load results and format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = []\n",
    "for _file_name in os.listdir(output_directory_path + '/sensitive-category-prediction'):\n",
    "    if _file_name.endswith('.csv') and _file_name.startswith('sensitive_categories.gpt4'):\n",
    "        df_list.append(pd.read_csv(output_directory_path + '/sensitive-category-prediction/' + _file_name))\n",
    "len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df = pd.concat(df_list)\n",
    "len(categories_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['sexually explicit or sexually suggestive content',\n",
    "              'copy-and-pasted emails or messaging transcripts',\n",
    "              'copy-and-pasted programming code longer than a single line',\n",
    "              'job, visa, and other applications',\n",
    "              'financial or corporate information',\n",
    "              'academic and educational information, including homework problems and essay writing',\n",
    "              'personal expressions about the user\\'s emotions, mental health, etc.',\n",
    "              'personal expressions about the user\\'s politics and religion',\n",
    "              'mention or discussion of the user\\'s personal relationships',\n",
    "              'medical and healthcare information',\n",
    "              'engagement with a specific fandom, including character development, story writing, and discussions related to the fandom',\n",
    "              'mention or discussion of the user\\'s hobbies and habits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(answer):\n",
    "    _found_categories = []\n",
    "    for _line in answer.split('\\n'):\n",
    "        for c in categories:\n",
    "            if _line.startswith(c) or _line[3:].startswith(c) or _line[4:].startswith(c):\n",
    "                _found_categories.append(c)\n",
    "    return _found_categories\n",
    "\n",
    "categories_df['predicted_categories'] = categories_df['answer'].apply(extract_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_confidence(answer):\n",
    "    _chunks = _answer.split(';')\n",
    "    if len(_chunks) == 5:\n",
    "        return _chunks[3]\n",
    "    return None\n",
    "\n",
    "categories_df['predicted_confidence'] = categories_df['answer'].apply(extract_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8959"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data_dict = defaultdict(list)\n",
    "data_dicts = []\n",
    "\n",
    "for i, r in categories_df.iterrows():\n",
    "\n",
    "    for _line in r['answer'].split('\\n'):\n",
    "         \n",
    "        _chunks = _line.split(';')\n",
    "\n",
    "        if len(_chunks) == 5:\n",
    "\n",
    "            _explicit = _chunks[1].strip()\n",
    "            _people = _chunks[2].strip()\n",
    "            _confidence = _chunks[3].strip()\n",
    "            _reason = _chunks[4].strip()\n",
    "\n",
    "            _category_prediction = _chunks[0].lower()\n",
    "            _category = None    \n",
    "            if 'sex' in _category_prediction:\n",
    "                _category = 'sexual and erotic content'\n",
    "            elif 'email' in _category_prediction or 'messag' in _category_prediction:\n",
    "                _category = 'quoted emails and messages'\n",
    "            elif 'program' in _category_prediction or 'code' in _category_prediction:\n",
    "                _category = 'quoted code'\n",
    "            elif 'job' in _category_prediction or 'application' in _category_prediction:\n",
    "                _category = 'job, visa, and other applications'\n",
    "            elif 'financ' in _category_prediction or 'corporate' in _category_prediction:\n",
    "                _category = 'financial and corporate info'\n",
    "            elif 'academic' in _category_prediction or 'education' in _category_prediction or 'homework' in _category_prediction or 'essay' in _category_prediction:\n",
    "                _category = 'academic and education info' \n",
    "            elif 'emotion' in _category_prediction or 'mental' in _category_prediction:\n",
    "                _category = 'user\\'s emotions and mental health' \n",
    "            elif 'politic' in _category_prediction or 'religion' in _category_prediction:\n",
    "                _category = 'user\\'s politics and religion' \n",
    "            elif 'relationship' in _category_prediction:\n",
    "                _category = 'user\\'s personal relationships' \n",
    "            elif 'medical' in _category_prediction or 'healthcare' in _category_prediction:\n",
    "                _category = 'healthcare information' \n",
    "            elif 'fandom' in _category_prediction:\n",
    "                _category = 'fandom' \n",
    "            elif 'hobb' in _category_prediction or 'habit' in _category_prediction:\n",
    "                _category = 'user\\'s hobbies and habits'\n",
    "\n",
    "            if _category:\n",
    "                conversation_data_dict[r['conversation_hash']].append({'conversation_hash': r['conversation_hash'],\n",
    "                                                                       'line': _line,\n",
    "                                                                       'predicted_explicit': _explicit,\n",
    "                                                                       'predicted_people': _people,\n",
    "                                                                       'predicted_confidence': _confidence,\n",
    "                                                                       'predicted_reason': _reason,\n",
    "                                                                       'predicted_category': _category})\n",
    "                \n",
    "                data_dicts.append({'conversation_hash': r['conversation_hash'],\n",
    "                                   'line': _line,\n",
    "                                   'predicted_explicit': _explicit,\n",
    "                                   'predicted_people': _people,\n",
    "                                   'predicted_confidence': _confidence,\n",
    "                                   'predicted_reason': _reason,\n",
    "                                   'predicted_category': _category})\n",
    "            \n",
    "sensitive_df = pd.DataFrame(data_dicts)\n",
    "len(sensitive_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_df.to_csv(output_directory_path + '/sensitive-category-prediction/sensitive_categories.gpt4.processed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
